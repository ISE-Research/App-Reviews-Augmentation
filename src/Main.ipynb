{"cells":[{"cell_type":"markdown","metadata":{"id":"lWNnDniynQpq"},"source":["# **Config Colab**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB8N0kwTm8N5"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","! pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"41g3dCa2nb0f"},"source":["**GPU**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXTvIgmZcqUI"},"outputs":[],"source":["import tensorflow as tf\n","tf.test.gpu_device_name()\n","\n","import os\n","import numpy as np\n","import random as rn\n","import tensorflow as tf\n","from transformers import set_seed\n","\n","seed = 1234\n","os.environ['PYTHONHASHSEED']= '0'\n","np.random.seed(seed)\n","rn.seed(seed)\n","tf.random.set_seed(seed)\n","set_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"PVklL7bBoC-Q"},"source":["# **Fine Tuning**"]},{"cell_type":"markdown","metadata":{"id":"e4KwNueS3KH1"},"source":["## **config**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQ4TY9XGoIda"},"outputs":[],"source":["import csv\n","import pandas as pd\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, precision_score, recall_score, f1_score, accuracy_score\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","def convert_digit_multi(input_labels, target_label, labels):\n","    other_labels = [y for y in labels if y != target_label]\n","    new_label = []\n","    for x in input_labels:\n","        if x == target_label:\n","            new_label.append(0)\n","        elif x == other_labels[0]:\n","            new_label.append(1)\n","        elif x == other_labels[1]:\n","            new_label.append(2)\n","    return new_label\n","\n","def convert_digit(input_labels, target_label, labels):\n","    other_labels = [y for y in labels if y != target_label]\n","    new_label = []\n","    for x in input_labels:\n","        if x == target_label:\n","            new_label.append(0)\n","        elif x in other_labels:\n","            new_label.append(1)\n","    return new_label\n","\n","def evaluate_bi(test_label, pred_label, confusion_m=False):\n","    # pred_label = model.predict(test_sent)\n","    if confusion_m:\n","        cm=confusion_matrix(test_label, pred_label)\n","        print(cm)\n","        # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n","        # disp.plot()\n","    target = 1\n","    precision = precision_score(test_label, pred_label, average='binary', pos_label=target)\n","    recall = recall_score(test_label, pred_label, average='binary', pos_label=target)\n","    f1 = f1_score(test_label, pred_label, average='binary', pos_label=target)\n","    accuracy = accuracy_score(test_label, pred_label)\n","    # print(f\"precision: {precision} \\trecal: {recall} \\tf1: {f1} \\taccuracy: {accuracy}\")\n","    return [precision, recall, f1, accuracy]\n","\n","def evaluate_multi(test_label, pred_label, avg, confusion_m=False):\n","    # pred_label = model.predict(test_sent)\n","    if confusion_m:\n","        cm=confusion_matrix(test_label, pred_label)\n","        print(cm)\n","        # disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n","        # disp.plot()\n","    precision = precision_score(test_label, pred_label, average=avg)\n","    recall = recall_score(test_label, pred_label, average=avg)\n","    f1 = f1_score(test_label, pred_label, average=avg)\n","    accuracy = accuracy_score(test_label, pred_label)\n","    # print(f\"precision: {precision} \\trecal: {recall} \\tf1: {f1} \\taccuracy: {accuracy}\")\n","    return [precision, recall, f1, accuracy]\n","\n","def to_class_names(predictions, target_label):\n","    class_names = [target_label, 'other']\n","    # predictions = tf.nn.softmax(predictions.logits)\n","    predictions_index = tf.argmax(predictions, axis=1).numpy()\n","    predictions = [class_names[prediction] for prediction in predictions_index]\n","    return predictions_index, predictions\n","\n","def to_label_index(predictions):\n","    predictions = tf.nn.softmax(predictions.logits)\n","    predictions = tf.argmax(predictions, axis=1).numpy()\n","    return predictions"]},{"cell_type":"markdown","metadata":{"id":"1Ux7jeeDtReP"},"source":["## **train on reviews+issues**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ca_BRGUuCBzP"},"outputs":[],"source":["# train on cobmination\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.utils import compute_class_weight\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","labels = ['bug', 'feature']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    for test_dataset_name in datasets:\n","\n","        test_labels = []\n","        pred_labels = []\n","        test_sents = []\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_dataset_name)\n","\n","        file_path = data_path+f\"{test_dataset_name}_{suffix}.csv\"\n","        test_dataset = pd.read_csv(file_path)\n","        test_dataset = test_dataset.fillna(\"\")\n","\n","        test_dataset = test_dataset[['app_name', 'text', tlabel]]\n","        test_dataset[tlabel] = test_dataset[tlabel].map(int)\n","        print(test_dataset[tlabel].value_counts())\n","\n","        all_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","            if dataset != test_dataset_name:\n","              result = []\n","              file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","              all_dataset = pd.concat([all_dataset, review_dataset])\n","\n","        all_dataset = all_dataset[['app_name', 'text', tlabel]]\n","        all_dataset[tlabel] = all_dataset[tlabel].map(int)\n","        print(\"first train dataset: \")\n","        print(all_dataset[tlabel].value_counts())\n","\n","        perc = 0.1\n","        sample_no = int(all_dataset.shape[0]*perc)\n","        all_issues_ = all_issues.sample(n=sample_no, random_state=seed)\n","        print(\"sample no: \", sample_no)\n","        dataset_train = pd.concat([all_dataset[['text', tlabel]], all_issues_[['text', tlabel]]])\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            # model.summary()\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                      callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent = test_dataset['text'].values.tolist()\n","            test_sents.extend(test_sent)\n","            test_sent = tokenizer(test_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_mic.append(result)\n","\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n","\n","        true_ids = [len(test_sents[i].split()) for i,x in enumerate(pred_labels) if x == 1 and test_labels[i] == 1]\n","        false_ids = [len(test_sents[i].split()) for i,x in enumerate(pred_labels) if x == 0 and test_labels[i] == 1]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DQ8gjxaGsC4g"},"source":["## **train on reviews**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QX3IbgcyUM8f"},"outputs":[],"source":["# train on reviews only\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.utils import compute_class_weight\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","labels = ['bug', 'feature']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","name = \"jan_2\"\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    for test_dataset_name in datasets:\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_dataset_name)\n","\n","        file_path = data_path+f\"{test_dataset_name}_{suffix}.csv\"\n","        test_dataset = pd.read_csv(file_path)\n","        test_dataset = test_dataset.fillna(\"\")\n","\n","        test_dataset = test_dataset[['app_name', 'text', tlabel]]\n","        test_dataset[tlabel] = test_dataset[tlabel].map(int)\n","        print(test_dataset[tlabel].value_counts())\n","\n","        all_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","            if dataset != test_dataset_name:\n","              result = []\n","              file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","              all_dataset = pd.concat([all_dataset, review_dataset])\n","\n","        all_dataset = all_dataset[['app_name', 'text', tlabel]]\n","        all_dataset[tlabel] = all_dataset[tlabel].map(int)\n","        print(\"first train dataset: \")\n","        print(all_dataset[tlabel].value_counts())\n","\n","        dataset_train = all_dataset[['text', tlabel]]\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            # model.summary()\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                      callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent = test_dataset['text'].values.tolist()\n","            test_sents.extend(test_sent)\n","            test_sent = tokenizer(test_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_mic.append(result)\n","\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PqbK7G04r4Od"},"source":["## **Same app and Simailar**"]},{"cell_type":"markdown","metadata":{"id":"v3X6gQ0TLc1h"},"source":["### **review**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgYpsYR_yP9T"},"outputs":[],"source":["\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils import compute_class_weight\n","\n","\n","\n","labels = ['bug', 'feature']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","pack_to_git = { 'fr.free.nrw.commons': 'commons-app/apps-android-commons',\n","                'org.mozilla.focus': 'mozilla-mobile/focus-android',\n","                'org.odk.collect.android': 'opendatakit/collect',\n","                'de.danoeh.antennapod': 'AntennaPod/AntennaPod',\n","                'com.habitrpg.android.habitica': 'HabitRPG/habitica-android',\n","                'com.ichi2.anki': 'ankidroid/Anki-Android',\n","                'io.metamask': 'MetaMask/metamask-mobile',\n","                'io.homeassistant.companion.android': 'home-assistant/home-assistant-android',\n","                'org.mozilla.fenix': 'mozilla-mobile/fenix',\n","                'com.owncloud.android': 'nextcloud/android',\n","                'me.ccrama.redditslide': 'ccrama/Slide',\n","                'net.cozic.joplin': 'laurent22/joplin',\n","                'com.tavultesoft.kmapro': 'keymanapp/keyman'}\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","\n","git_basic_details = pd.read_csv(data_path+'git_details_proc.csv')\n","git_basic_details = git_basic_details.fillna(\"\")\n","git_basic_descriptions = git_basic_details['text'].values.tolist()\n","git_basic_names = git_basic_details['id'].values\n","\n","desc_tfIdfVectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), use_idf=True)\n","tfidf = desc_tfIdfVectorizer.fit_transform(git_basic_descriptions)\n","git_basic_descriptions_vectors = desc_tfIdfVectorizer.transform(git_basic_descriptions).toarray()\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","\n","no_similars = 20\n","no_sample = 2000\n","no_app = 1\n","\n","review_file = data_path + 'z_sample_reviews_2_f_pre_4.csv'\n","all_reviews = pd.read_csv(review_file)\n","all_reviews = all_reviews.fillna(\"\")\n","\n","app_names = np.unique(all_reviews['app_id'].values)\n","\n","\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","\n","    all_review_dataset = pd.DataFrame([])\n","    for dataset in datasets:\n","        result = []\n","        file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","        review_dataset = pd.read_csv(file_path)\n","        review_dataset = review_dataset.fillna(\"\")\n","        all_review_dataset = pd.concat([all_review_dataset, review_dataset])\n","\n","    all_review_dataset = all_review_dataset[['app_name', 'text', tlabel]]\n","    all_review_dataset[tlabel] = all_review_dataset[tlabel].map(int)\n","\n","    dataset_train = all_review_dataset[['text', tlabel]]\n","\n","    print(\"train dataset: \")\n","    print(dataset_train[tlabel].value_counts())\n","\n","    set_x = dataset_train['text'].values.tolist()\n","    set_y = dataset_train[tlabel].values.tolist()\n","\n","    k = 5\n","    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","    kfold = kfold.split(set_x, set_y)\n","\n","    i = 0\n","    result_pre = []\n","    result_rec = []\n","    result_f1 = []\n","    result_mic = []\n","\n","    for train_index, val_index in kfold:\n","        print(\"------------------------------------------------------\")\n","        print(\"repeat: \", i)\n","        i += 1\n","        np.random.shuffle(train_index)\n","        np.random.shuffle(val_index)\n","        train_sent = [set_x[i] for i in train_index]\n","        train_label = [set_y[i] for i in train_index]\n","        val_sent = [set_x[i] for i in val_index]\n","        val_label = [set_y[i] for i in val_index]\n","\n","        tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","        train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","        val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","        train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","        val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","        model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","        # model.layers[0].trainable = False\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        metrics=tf.metrics.SparseCategoricalAccuracy()\n","        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","        # model.summary()\n","        model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                  callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","\n","        for test_name in app_names:\n","            print(\"test on: \", test_name)\n","            test_dataset = all_reviews[all_reviews['app_id'] == test_name][['text_pre', 'label_b', 'label_f']]\n","            print(test_dataset[tlabel].value_counts())\n","\n","            test_sent_ = test_dataset['text_pre'].values.tolist()\n","            test_sents.extend(test_sent_)\n","            test_sent = tokenizer(test_sent_, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result_mic.append(result)\n","\n","    i = -1\n","    result_avg = []\n","    step = len(datasets) - 1\n","    for test_name in app_names:\n","        i += 1\n","        print(\"****************************************\")\n","        print(test_name)\n","        result_pre_ = []\n","        result_rec_ = []\n","        result_f1_ = []\n","        result_mic_ = []\n","\n","        for j in range(k):\n","            result_pre_.append(result_pre[step*j+i])\n","            result_rec_.append(result_rec[step*j+i])\n","            result_f1_.append(result_f1[step*j+i])\n","            result_mic_.append(result_mic[step*j+i])\n","\n","        df_pre = pd.DataFrame(result_pre_, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec_, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1_, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic_, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n","\n","        result_avg.append(df_micro.loc['mean'].values.tolist())\n","\n","\n","    df_result_avg = pd.DataFrame(result_avg, columns=['p', 'r', 'f1', 'ac'], index=app_names)\n","    df_result_avg.loc['mean'] = df_result_avg.mean()\n","    print(df_result_avg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xgZP4TzvSNpL"},"source":["### **review+same**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6geENeOSQpd"},"outputs":[],"source":["\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils import compute_class_weight\n","\n","\n","\n","\n","labels = ['bug', 'feature', 'other']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","pack_to_git = { 'fr.free.nrw.commons': 'commons-app/apps-android-commons',\n","                'org.mozilla.focus': 'mozilla-mobile/focus-android',\n","                'org.odk.collect.android': 'opendatakit/collect',\n","                'de.danoeh.antennapod': 'AntennaPod/AntennaPod',\n","                'com.habitrpg.android.habitica': 'HabitRPG/habitica-android',\n","                'com.ichi2.anki': 'ankidroid/Anki-Android',\n","                'io.metamask': 'MetaMask/metamask-mobile',\n","                'io.homeassistant.companion.android': 'home-assistant/home-assistant-android',\n","                'org.mozilla.fenix': 'mozilla-mobile/fenix',\n","                'com.owncloud.android': 'nextcloud/android',\n","                'me.ccrama.redditslide': 'ccrama/Slide',\n","                'net.cozic.joplin': 'laurent22/joplin',\n","                'com.tavultesoft.kmapro': 'keymanapp/keyman'}\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","\n","git_basic_details = pd.read_csv(data_path+'git_details_proc.csv')\n","git_basic_details = git_basic_details.fillna(\"\")\n","git_basic_descriptions = git_basic_details['text'].values.tolist()\n","git_basic_names = git_basic_details['id'].values\n","\n","desc_tfIdfVectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), use_idf=True)\n","tfidf = desc_tfIdfVectorizer.fit_transform(git_basic_descriptions)\n","git_basic_descriptions_vectors = desc_tfIdfVectorizer.transform(git_basic_descriptions).toarray()\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","\n","no_similars = 20\n","no_sample = 2000\n","no_app = 1\n","\n","review_file = data_path + 'z_sample_reviews_2_f_pre_4.csv'\n","all_reviews = pd.read_csv(review_file)\n","all_reviews = all_reviews.fillna(\"\")\n","\n","app_names = np.unique(all_reviews['app_id'].values)\n","\n","\n","for target_label in labels[:1]:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    for test_name in app_names[:2]:\n","\n","        test_labels = []\n","        pred_labels = []\n","        test_sents = []\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_name)\n","\n","        test_dataset = all_reviews[all_reviews['app_id'] == test_name][['text_pre', 'label_b', 'label_f']]\n","        print(test_dataset[tlabel].value_counts())\n","\n","        ############################## review datasets #########################\n","        all_review_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","              result = []\n","               file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","               all_review_dataset = pd.concat([all_review_dataset, review_dataset])\n","\n","        all_review_dataset = all_review_dataset[['app_name', 'text', tlabel]]\n","        all_review_dataset[tlabel] = all_review_dataset[tlabel].map(int)\n","\n","        ############################## similar git datasets #########################\n","\n","        git_app_description = git_basic_details[git_basic_details['id'] == pack_to_git[test_name]]['text'].values.tolist()[0]\n","        google_app_description_vector = desc_tfIdfVectorizer.transform([git_app_description]).toarray()[0]\n","\n","\n","        no_similars_ = no_similars\n","        similarity = [cosine_similarity([t, google_app_description_vector])[0][1] for t in git_basic_descriptions_vectors]\n","        similar_projects = git_basic_names[(-np.array(similarity)).argsort()[:no_similars_]]\n","\n","        not_exist = True\n","        tlabel = \"label_\"+target_label[0]\n","        similar_issues = all_issues[all_issues['id'].isin(similar_projects)][['text', tlabel]][:no_sample]\n","\n","        ############################## same git dataset #########################\n","        same_issues = all_issues[all_issues['id'] == pack_to_git[test_name]]\n","\n","        ############################## merge datasets #########################\n","        thr = int(all_review_dataset.shape[0]*0.3)\n","        if same_issues.shape[0] > thr:\n","            same_issues = same_issues.sample(n=thr, random_state=seed)\n","\n","        dataset_train = pd.concat([all_review_dataset[['text', tlabel]], same_issues[['text', tlabel]]])\n","\n","        print(\"same dataset: \")\n","        print(same_issues[tlabel].value_counts())\n","\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            # model.summary()\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                      callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent = test_dataset['text_pre'].values.tolist()\n","            test_sents.extend(test_sent)\n","            test_sent = tokenizer(test_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_mic.append(result)\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5WUgUktR15Jg"},"source":["### **review+similar**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFnArhxxPsuP"},"outputs":[],"source":["\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils import compute_class_weight\n","\n","\n","labels = ['bug', 'feature', 'other']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","pack_to_git = { 'fr.free.nrw.commons': 'commons-app/apps-android-commons',\n","                'org.mozilla.focus': 'mozilla-mobile/focus-android',\n","                'org.odk.collect.android': 'opendatakit/collect',\n","                'de.danoeh.antennapod': 'AntennaPod/AntennaPod',\n","                'com.habitrpg.android.habitica': 'HabitRPG/habitica-android',\n","                'com.ichi2.anki': 'ankidroid/Anki-Android',\n","                'io.metamask': 'MetaMask/metamask-mobile',\n","                'io.homeassistant.companion.android': 'home-assistant/home-assistant-android',\n","                'org.mozilla.fenix': 'mozilla-mobile/fenix',\n","                'com.owncloud.android': 'nextcloud/android',\n","                'me.ccrama.redditslide': 'ccrama/Slide',\n","                'net.cozic.joplin': 'laurent22/joplin',\n","                'com.tavultesoft.kmapro': 'keymanapp/keyman'}\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","\n","git_basic_details = pd.read_csv(data_path+'git_details_proc.csv')\n","git_basic_details = git_basic_details.fillna(\"\")\n","git_basic_descriptions = git_basic_details['text'].values.tolist()\n","git_basic_names = git_basic_details['id'].values\n","\n","desc_tfIdfVectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), use_idf=True)\n","tfidf = desc_tfIdfVectorizer.fit_transform(git_basic_descriptions)\n","git_basic_descriptions_vectors = desc_tfIdfVectorizer.transform(git_basic_descriptions).toarray()\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","\n","no_similars = 20\n","no_sample = 2000\n","no_app = 1\n","\n","review_file = data_path + 'z_sample_reviews_2_f_pre_4.csv'\n","all_reviews = pd.read_csv(review_file)\n","all_reviews = all_reviews.fillna(\"\")\n","\n","app_names = np.unique(all_reviews['app_id'].values)\n","\n","\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    for test_name in app_names:\n","\n","        test_labels = []\n","        pred_labels = []\n","        test_sents = []\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_name)\n","\n","        test_dataset = all_reviews[all_reviews['app_id'] == test_name][['text_pre', 'label_b', 'label_f']]\n","        print(test_dataset[tlabel].value_counts())\n","\n","        ############################## review datasets #########################\n","        all_review_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","              result = []\n","              file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","              all_review_dataset = pd.concat([all_review_dataset, review_dataset])\n","\n","        all_review_dataset = all_review_dataset[['app_name', 'text', tlabel]]\n","        all_review_dataset[tlabel] = all_review_dataset[tlabel].map(int)\n","\n","        ############################## similar git datasets #########################\n","        git_app_description = git_basic_details[git_basic_details['id'] == pack_to_git[test_name]]['text'].values.tolist()[0]\n","        google_app_description_vector = desc_tfIdfVectorizer.transform([git_app_description]).toarray()[0]\n","\n","\n","        no_similars_ = no_similars\n","        similarity = [cosine_similarity([t, google_app_description_vector])[0][1] for t in git_basic_descriptions_vectors]\n","        similar_projects = git_basic_names[(-np.array(similarity)).argsort()]\n","        similar_projects = [x for x in similar_projects if x != pack_to_git[test_name]]\n","\n","        not_exist = True\n","        tlabel = \"label_\"+target_label[0]\n","        thr = int(all_review_dataset.shape[0]*0.3)\n","        print(\"theresholad\", thr)\n","        similar_issues = pd.DataFrame([])\n","        for sim in similar_projects:\n","            similar_issues_ = all_issues[all_issues['id'] == sim][['text', tlabel]]\n","            similar_issues = pd.concat([similar_issues, similar_issues_])\n","            if similar_issues.shape[0] > thr:\n","                similar_issues = similar_issues.sample(n=thr, random_state=seed)\n","                break\n","\n","        ############################## same git dataset #########################\n","        same_issues = all_issues[all_issues['id'] == pack_to_git[test_name]]\n","\n","        ############################## merge datasets #########################\n","        dataset_train = pd.concat([all_review_dataset[['text', tlabel]], similar_issues[['text', tlabel]]])\n","\n","\n","        print(\"similar dataset: \")\n","        print(similar_issues[tlabel].value_counts())\n","\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                      callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent_ = test_dataset['text_pre'].values.tolist()\n","            test_sents.extend(test_sent_)\n","            test_sent = tokenizer(test_sent_, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result_mic.append(result)\n","            false_sents = [test_sent_[i] for i,x in enumerate(pred_label) if x == 0 and test_label[i] == 1]\n","            print(false_sents)\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n"]},{"cell_type":"markdown","metadata":{"id":"baAI2zXtPCVo"},"source":["### **review+same+similar**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnWD3uvVPYND"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils import compute_class_weight\n","\n","\n","labels = ['bug', 'feature', 'other']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","pack_to_git = { 'fr.free.nrw.commons': 'commons-app/apps-android-commons',\n","                'org.mozilla.focus': 'mozilla-mobile/focus-android',\n","                'org.odk.collect.android': 'opendatakit/collect',\n","                'de.danoeh.antennapod': 'AntennaPod/AntennaPod',\n","                'com.habitrpg.android.habitica': 'HabitRPG/habitica-android',\n","                'com.ichi2.anki': 'ankidroid/Anki-Android',\n","                'io.metamask': 'MetaMask/metamask-mobile',\n","                'io.homeassistant.companion.android': 'home-assistant/home-assistant-android',\n","                'org.mozilla.fenix': 'mozilla-mobile/fenix',\n","                'com.owncloud.android': 'nextcloud/android',\n","                'me.ccrama.redditslide': 'ccrama/Slide',\n","                'net.cozic.joplin': 'laurent22/joplin',\n","                'com.tavultesoft.kmapro': 'keymanapp/keyman'}\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","\n","git_basic_details = pd.read_csv(data_path+'git_details_proc.csv')\n","git_basic_details = git_basic_details.fillna(\"\")\n","git_basic_descriptions = git_basic_details['text'].values.tolist()\n","git_basic_names = git_basic_details['id'].values\n","\n","desc_tfIdfVectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), use_idf=True)\n","tfidf = desc_tfIdfVectorizer.fit_transform(git_basic_descriptions)\n","git_basic_descriptions_vectors = desc_tfIdfVectorizer.transform(git_basic_descriptions).toarray()\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","\n","no_similars = 20\n","no_sample = 2000\n","no_app = 1\n","\n","review_file = data_path + 'z_sample_reviews_2_f_pre_4.csv'\n","all_reviews = pd.read_csv(review_file)\n","all_reviews = all_reviews.fillna(\"\")\n","\n","app_names = np.unique(all_reviews['app_id'].values)\n","\n","\n","\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    start = 0\n","    for test_name in app_names[start:]:\n","\n","        test_labels = []\n","        pred_labels = []\n","        test_sents = []\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_name)\n","\n","        test_dataset = all_reviews[all_reviews['app_id'] == test_name][['text_pre', 'label_b', 'label_f']]\n","        print(test_dataset[tlabel].value_counts())\n","\n","        ############################## review datasets #########################\n","        all_review_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","              result = []\n","              file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","              all_review_dataset = pd.concat([all_review_dataset, review_dataset])\n","\n","        all_review_dataset = all_review_dataset[['app_name', 'text', tlabel]]\n","        all_review_dataset[tlabel] = all_review_dataset[tlabel].map(int)\n","\n","        ############################## similar git datasets #########################\n","        git_app_description = git_basic_details[git_basic_details['id'] == pack_to_git[test_name]]['text'].values.tolist()[0]\n","        google_app_description_vector = desc_tfIdfVectorizer.transform([git_app_description]).toarray()[0]\n","\n","\n","        no_similars_ = no_similars + 1\n","        similarity = [cosine_similarity([t, google_app_description_vector])[0][1] for t in git_basic_descriptions_vectors]\n","        similar_projects = git_basic_names[(-np.array(similarity)).argsort()]\n","        similar_projects = [x for x in similar_projects if x != pack_to_git[test_name]]\n","\n","        not_exist = True\n","        tlabel = \"label_\"+target_label[0]\n","        similar_issues = all_issues[all_issues['id'].isin(similar_projects)][['text', tlabel]]\n","\n","        ############################## same git dataset #########################\n","        same_issues = all_issues[all_issues['id'] == pack_to_git[test_name]]\n","\n","        ############################## merge datasets #########################\n","        thr = int(all_review_dataset.shape[0]*0.3)\n","        if similar_issues.shape[0] + same_issues.shape[0] > thr:\n","            thr = thr - same_issues.shape[0]\n","            similar_issues = pd.DataFrame([])\n","            for sim in similar_projects:\n","                similar_issues_ = all_issues[all_issues['id'] == sim][['text', tlabel]]\n","                similar_issues = pd.concat([similar_issues, similar_issues_])\n","                if similar_issues.shape[0] > thr:\n","                    similar_issues = similar_issues.sample(n=thr, random_state=seed)\n","                    break\n","\n","        aug_dataset = pd.concat([same_issues, similar_issues])\n","        dataset_train = pd.concat([all_review_dataset[['text', tlabel]], aug_dataset])\n","\n","\n","        print(\"same dataset: \")\n","        print(same_issues[tlabel].value_counts())\n","\n","        print(\"similar dataset: \")\n","        print(similar_issues[tlabel].value_counts())\n","\n","        print(\"aug dataset: \")\n","        print(aug_dataset[tlabel].value_counts())\n","\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                      callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent = test_dataset['text_pre'].values.tolist()\n","            test_sents.extend(test_sent)\n","            test_sent = tokenizer(test_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_mic.append(result)\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n"]},{"cell_type":"markdown","metadata":{"id":"0CWpXjGDNw3s"},"source":["### **random**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sErSNIwBOEdg"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils import compute_class_weight\n","\n","\n","labels = ['bug', 'feature', 'other']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/Colab Notebooks (1)/data/inter_context/\"\n","\n","pack_to_git = { 'fr.free.nrw.commons': 'commons-app/apps-android-commons',\n","                'org.mozilla.focus': 'mozilla-mobile/focus-android',\n","                'org.odk.collect.android': 'opendatakit/collect',\n","                'de.danoeh.antennapod': 'AntennaPod/AntennaPod',\n","                'com.habitrpg.android.habitica': 'HabitRPG/habitica-android',\n","                'com.ichi2.anki': 'ankidroid/Anki-Android',\n","                'io.metamask': 'MetaMask/metamask-mobile',\n","                'io.homeassistant.companion.android': 'home-assistant/home-assistant-android',\n","                'org.mozilla.fenix': 'mozilla-mobile/fenix',\n","                'com.owncloud.android': 'nextcloud/android',\n","                'me.ccrama.redditslide': 'ccrama/Slide',\n","                'net.cozic.joplin': 'laurent22/joplin',\n","                'com.tavultesoft.kmapro': 'keymanapp/keyman'}\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","\n","git_basic_details = pd.read_csv(data_path+'git_details_proc.csv')\n","git_basic_details = git_basic_details.fillna(\"\")\n","git_basic_descriptions = git_basic_details['text'].values.tolist()\n","git_basic_names = git_basic_details['id'].values\n","\n","desc_tfIdfVectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), use_idf=True)\n","tfidf = desc_tfIdfVectorizer.fit_transform(git_basic_descriptions)\n","git_basic_descriptions_vectors = desc_tfIdfVectorizer.transform(git_basic_descriptions).toarray()\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","\n","no_similars = 20\n","no_sample = 2000\n","no_app = 1\n","\n","review_file = data_path + 'z_sample_reviews_2_f_pre.csv'\n","all_reviews = pd.read_csv(review_file)\n","all_reviews = all_reviews.fillna(\"\")\n","\n","app_names = np.unique(all_reviews['app_id'].values)\n","\n","\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    for test_name in app_names:\n","\n","        test_labels = []\n","        pred_labels = []\n","        test_sents = []\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_name)\n","\n","        test_dataset = all_reviews[all_reviews['app_id'] == test_name][['text_pre', 'label_b', 'label_f']]\n","        print(test_dataset[tlabel].value_counts())\n","\n","        ############################## review datasets #########################\n","        all_review_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","              result = []\n","              file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","              all_review_dataset = pd.concat([all_review_dataset, review_dataset])\n","\n","        all_review_dataset = all_review_dataset[['app_name', 'text', tlabel]]\n","        all_review_dataset[tlabel] = all_review_dataset[tlabel].map(int)\n","\n","        ############################## similar git datasets #########################\n","\n","        git_app_description = git_basic_details[git_basic_details['id'] == pack_to_git[test_name]]['text'].values.tolist()[0]\n","        google_app_description_vector = desc_tfIdfVectorizer.transform([git_app_description]).toarray()[0]\n","\n","\n","        no_similars_ = no_similars\n","        similarity = [cosine_similarity([t, google_app_description_vector])[0][1] for t in git_basic_descriptions_vectors]\n","        similar_projects = git_basic_names[(-np.array(similarity)).argsort()[:no_similars_]]\n","\n","        not_exist = True\n","        tlabel = \"label_\"+target_label[0]\n","        similar_issues = all_issues[all_issues['id'].isin(similar_projects)][['text', tlabel]]\n","\n","        ############################## same git dataset #########################\n","        same_issues = all_issues[all_issues['id'] == pack_to_git[test_name]]\n","\n","        ############################## merge datasets #########################\n","        perc = 0.1\n","        thr = int(all_review_dataset.shape[0]*perc)\n","        aug_dataset = all_issues.sample(n=thr, random_state=seed)\n","\n","        dataset_train = pd.concat([all_review_dataset[['text', tlabel]], aug_dataset[['text', tlabel]]])\n","        print(\"aug dataset: \")\n","        print(aug_dataset[tlabel].value_counts())\n","\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            # model.summary()\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                      callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent = test_dataset['text_pre'].values.tolist()\n","            test_sents.extend(test_sent)\n","            test_sent = tokenizer(test_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_mic.append(result)\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n","\n",""]},{"cell_type":"markdown","metadata":{"id":"XBwjHoVTVNqU"},"source":["### **similar**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29TGWpReVVn8"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFAutoModelForSequenceClassification\n","from keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.utils import compute_class_weight\n","\n","\n","labels = ['bug', 'feature', 'other']\n","datasets = ['guzman_dataset', 'maalej_dataset', 'jha_dataset', 'surminer_dataset','scalabrino_dataset']\n","data_path = \"/content/drive/MyDrive/ColabNotebooks/data/inter_context/\"\n","\n","pack_to_git = { 'fr.free.nrw.commons': 'commons-app/apps-android-commons',\n","                'org.mozilla.focus': 'mozilla-mobile/focus-android',\n","                'org.odk.collect.android': 'opendatakit/collect',\n","                'de.danoeh.antennapod': 'AntennaPod/AntennaPod',\n","                'com.habitrpg.android.habitica': 'HabitRPG/habitica-android',\n","                'com.ichi2.anki': 'ankidroid/Anki-Android',\n","                'io.metamask': 'MetaMask/metamask-mobile',\n","                'io.homeassistant.companion.android': 'home-assistant/home-assistant-android',\n","                'org.mozilla.fenix': 'mozilla-mobile/fenix',\n","                'com.owncloud.android': 'nextcloud/android',\n","                'me.ccrama.redditslide': 'ccrama/Slide',\n","                'net.cozic.joplin': 'laurent22/joplin',\n","                'com.tavultesoft.kmapro': 'keymanapp/keyman'}\n","\n","deep_model = 'distilbert-base-uncased'\n","b_size = 16\n","l_r = 5e-5\n","epoch_no = 3\n","min_delta = 0.0001\n","patience = 5\n","num_labels = 2\n","max_length = 50\n","suffix = \"f\"\n","\n","test_labels = []\n","pred_labels = []\n","test_sents = []\n","\n","git_basic_details = pd.read_csv(data_path+'git_details_proc.csv')\n","git_basic_details = git_basic_details.fillna(\"\")\n","git_basic_descriptions = git_basic_details['text'].values.tolist()\n","git_basic_names = git_basic_details['id'].values\n","\n","desc_tfIdfVectorizer = TfidfVectorizer(min_df=5, ngram_range=(1, 2), use_idf=True)\n","tfidf = desc_tfIdfVectorizer.fit_transform(git_basic_descriptions)\n","git_basic_descriptions_vectors = desc_tfIdfVectorizer.transform(git_basic_descriptions).toarray()\n","\n","\n","name = \"jan_2\"\n","all_issues = pd.read_csv(data_path+f\"z_all_issues_train_{name}.csv\",\n","                         names=['id', 'text', 'label_b', 'label_f'])[1:]\n","all_issues = all_issues.fillna(\"\")\n","all_issues['label_b'] = all_issues['label_b'].map(int)\n","all_issues['label_f'] = all_issues['label_f'].map(int)\n","\n","\n","no_similars = 20\n","no_sample = 2000\n","no_app = 1\n","\n","review_file = data_path + 'z_sample_reviews_2_f_pre_4.csv'\n","all_reviews = pd.read_csv(review_file)\n","all_reviews = all_reviews.fillna(\"\")\n","app_names = np.unique(all_reviews['app_id'].values)\n","\n","\n","for target_label in labels:\n","    print(f\"--------------------------- target label: {target_label} ---------------------------\")\n","    tlabel = 'label_'+target_label[0]\n","    for test_name in app_names:\n","\n","        test_labels = []\n","        pred_labels = []\n","        test_sents = []\n","\n","        print(\"***************************************************\")\n","        print('test on: ', test_name)\n","\n","        test_dataset = all_reviews[all_reviews['app_id'] == test_name][['text_pre', 'label_b', 'label_f']]\n","        print(test_dataset[tlabel].value_counts())\n","\n","        ############################## review datasets #########################\n","        all_review_dataset = pd.DataFrame([])\n","        for dataset in datasets:\n","              result = []\n","              file_path = data_path+f\"{dataset}_{suffix}.csv\"\n","              review_dataset = pd.read_csv(file_path)\n","              review_dataset = review_dataset.fillna(\"\")\n","              all_review_dataset = pd.concat([all_review_dataset, review_dataset])\n","\n","        all_review_dataset = all_review_dataset[['app_name', 'text', tlabel]]\n","        all_review_dataset[tlabel] = all_review_dataset[tlabel].map(int)\n","\n","        ############################## similar git datasets #########################\n","        git_app_description = git_basic_details[git_basic_details['id'] == pack_to_git[test_name]]['text'].values.tolist()[0]\n","        google_app_description_vector = desc_tfIdfVectorizer.transform([git_app_description]).toarray()[0]\n","\n","\n","        no_similars_ = no_similars\n","        similarity = [cosine_similarity([t, google_app_description_vector])[0][1] for t in git_basic_descriptions_vectors]\n","        similar_projects = git_basic_names[(-np.array(similarity)).argsort()]\n","        similar_projects = [x for x in similar_projects if x != pack_to_git[test_name]]\n","\n","        not_exist = True\n","        tlabel = \"label_\"+target_label[0]\n","        thr = int(all_review_dataset.shape[0]*0.3)\n","        print(\"theresholad\", thr)\n","        similar_issues = pd.DataFrame([])\n","        for sim in similar_projects:\n","            similar_issues_ = all_issues[all_issues['id'] == sim][['text', tlabel]]\n","            similar_issues = pd.concat([similar_issues_, review_dataset])\n","            if similar_issues.shape[0] > thr:\n","                similar_issues = similar_issues.sample(n=thr, random_state=seed)\n","                break\n","        ############################## same git dataset #########################\n","        same_issues = all_issues[all_issues['id'] == pack_to_git[test_name]]\n","\n","        ############################## merge datasets #########################\n","        dataset_train = pd.concat([all_review_dataset[['text', tlabel]], similar_issues[['text', tlabel]]])\n","\n","        print(\"similar dataset: \")\n","        print(similar_issues[tlabel].value_counts())\n","\n","        print(\"train dataset: \")\n","        print(dataset_train[tlabel].value_counts())\n","\n","        set_x = dataset_train['text'].values.tolist()\n","        set_y = dataset_train[tlabel].values.tolist()\n","\n","        k = 5\n","        kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n","        kfold = kfold.split(set_x, set_y)\n","\n","        i = 0\n","        result_pre = []\n","        result_rec = []\n","        result_f1 = []\n","        result_mic = []\n","        for train_index, val_index in kfold:\n","            print(\"------------------------------------------------------\")\n","            print(\"repeat: \", i)\n","            i += 1\n","            np.random.shuffle(train_index)\n","            np.random.shuffle(val_index)\n","            train_sent = [set_x[i] for i in train_index]\n","            train_label = [set_y[i] for i in train_index]\n","            val_sent = [set_x[i] for i in val_index]\n","            val_label = [set_y[i] for i in val_index]\n","\n","\n","            tokenizer = AutoTokenizer.from_pretrained(deep_model)\n","            train_sent_ = tokenizer(train_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","            val_sent_ = tokenizer(val_sent, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","            train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_sent_), train_label)).batch(b_size)\n","            val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_sent_), val_label)).batch(b_size)\n","\n","            model = TFAutoModelForSequenceClassification.from_pretrained(deep_model, num_labels=num_labels)\n","            # model.layers[0].trainable = False\n","            optimizer = tf.keras.optimizers.Adam(learning_rate=l_r)\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","            metrics=tf.metrics.SparseCategoricalAccuracy()\n","            model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","            # model.summary()\n","            model.fit(train_dataset, validation_data=val_dataset, epochs=epoch_no, batch_size=b_size,\n","                       callbacks=[EarlyStopping(monitor='val_loss', verbose=1, patience=patience, min_delta=min_delta)])\n","\n","            test_sent_ = test_dataset['text_pre'].values.tolist()\n","            test_sents.extend(test_sent_)\n","            test_sent = tokenizer(test_sent_, padding=\"max_length\", max_length=max_length, truncation=True)\n","            test_label = test_dataset[tlabel].values.tolist()\n","\n","            pred_label = model.predict([x.ids for x in test_sent[::]])\n","            pred_label = to_label_index(pred_label)\n","\n","            test_labels.extend(test_label)\n","            pred_labels.extend(pred_label)\n","\n","            result = evaluate_bi(test_label, pred_label, True)\n","            result_pre.append(result[0])\n","            result_rec.append(result[1])\n","            result_f1.append(result[2])\n","            result_mic.append(result)\n","            false_sents = [test_sent_[i] for i,x in enumerate(pred_label) if x == 0 and test_label[i] == 1]\n","            print(false_sents)\n","\n","        df_pre = pd.DataFrame(result_pre, index = range(k), columns=[target_label])\n","        df_pre.loc['mean'] = df_pre.mean()\n","        print(\"precision: \")\n","        print(df_pre)\n","\n","        df_rec = pd.DataFrame(result_rec, index = range(k), columns=[target_label])\n","        df_rec.loc['mean'] = df_rec.mean()\n","        print(\"recall: \")\n","        print(df_rec)\n","\n","        df_f1 = pd.DataFrame(result_f1, index = range(k), columns=[target_label])\n","        df_f1.loc['mean'] = df_f1.mean()\n","        print(\"f1-measure: \")\n","        print(df_f1)\n","\n","        df_micro = pd.DataFrame(result_mic, index = range(k), columns=['precision', 'recall', 'f1-meature', 'accuracy'])\n","        df_micro.loc['mean'] = df_micro.mean()\n","        print(\"micro: \\n\")\n","        print(df_micro)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}